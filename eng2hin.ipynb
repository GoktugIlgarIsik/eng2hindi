{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-26T13:15:24.705157Z",
     "iopub.status.busy": "2025-07-26T13:15:24.704508Z",
     "iopub.status.idle": "2025-07-26T13:21:21.819625Z",
     "shell.execute_reply": "2025-07-26T13:21:21.818924Z",
     "shell.execute_reply.started": "2025-07-26T13:15:24.705135Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2811/14065 [01:09<04:22, 42.82it/s, loss=4.5258, epoch=2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 2 validation loss: 4.5513\n",
      " Model saved as model_epoch2.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 5626/14065 [02:19<03:19, 42.21it/s, loss=3.6555, epoch=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 3 validation loss: 4.1141\n",
      " Model saved as model_epoch3.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 8441/14065 [03:29<15:56,  5.88it/s, loss=2.8080, epoch=4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 4 validation loss: 3.8948\n",
      " Model saved as model_epoch4.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 11251/14065 [04:38<01:06, 42.30it/s, loss=3.1244, epoch=5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 5 validation loss: 3.7765\n",
      " Model saved as model_epoch5.pt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14065/14065 [05:45<00:00, 40.68it/s, loss=2.7607, epoch=5]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"Aarif1430/english-to-hindi\")\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return sentence.lower().strip().split()\n",
    "\n",
    "def build_vocab(sentences, max_vocab_size=10000):\n",
    "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
    "    word_freq = {}\n",
    "    for sent in sentences:\n",
    "        for word in tokenize(sent):\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, _ in sorted_words[:max_vocab_size - len(vocab)]:\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "eng_sentences = [item['english_sentence'] for item in train_data]\n",
    "hi_sentences  = [item['hindi_sentence'] for item in train_data]\n",
    "eng_vocab = build_vocab(eng_sentences)\n",
    "hi_vocab  = build_vocab(hi_sentences)\n",
    "\n",
    "\n",
    "MAX_LEN = 30\n",
    "\n",
    "def encode_sentence(sentence, vocab):\n",
    "    tokens = [\"<sos>\"] + tokenize(sentence)[:MAX_LEN-2] + [\"<eos>\"]\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "def pad_tensor(tokens, max_len=MAX_LEN):\n",
    "    padded = tokens + [0] * (max_len - len(tokens))\n",
    "    return torch.tensor(padded[:max_len])\n",
    "\n",
    "src_data = [pad_tensor(encode_sentence(s, eng_vocab)) for s in eng_sentences[:100000]]\n",
    "trg_data = [pad_tensor(encode_sentence(s, hi_vocab)) for s in hi_sentences[:100000]]\n",
    "src_tensor = torch.stack(src_data)\n",
    "trg_tensor = torch.stack(trg_data)\n",
    "dataset = TensorDataset(src_tensor, trg_tensor)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, emb_dim=256, hidden_dim=512, num_layers=1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, emb_dim)\n",
    "        self.encoder_lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, emb_dim)\n",
    "        self.decoder_lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        embedded_src = self.encoder_embedding(src)\n",
    "        _, (hidden, cell) = self.encoder_lstm(embedded_src)\n",
    "        embedded_trg = self.decoder_embedding(trg)\n",
    "        output, _ = self.decoder_lstm(embedded_trg, (hidden, cell))\n",
    "        predictions = self.fc_out(output)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(len(eng_vocab), len(hi_vocab)).to(device)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.95)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "epochs = 5\n",
    "total_steps = epochs * len(train_loader)\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "pbar = tqdm(range(1, total_steps + 1))\n",
    "\n",
    "for step in pbar:\n",
    "    try:\n",
    "        src_batch, trg_batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_src, val_trg in val_loader:\n",
    "                val_src, val_trg = val_src.to(device), val_trg.to(device)\n",
    "                val_output = model(val_src, val_trg[:, :-1])\n",
    "                loss = loss_fn(val_output.reshape(-1, val_output.shape[-1]), val_trg[:, 1:].reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        current_epoch = (step - 1) // len(train_loader) + 1\n",
    "        print(f\"\\n Epoch {current_epoch} validation loss: {avg_val_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"model_epoch{current_epoch}.pt\")\n",
    "        print(f\" Model saved as model_epoch{current_epoch}.pt\\n\")\n",
    "        model.train()\n",
    "        data_iter = iter(train_loader)\n",
    "        src_batch, trg_batch = next(data_iter)\n",
    "\n",
    "    src_batch, trg_batch = src_batch.to(device), trg_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src_batch, trg_batch[:, :-1])\n",
    "    loss = loss_fn(output.reshape(-1, output.shape[-1]), trg_batch[:, 1:].reshape(-1))\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    current_epoch = (step - 1) // len(train_loader) + 1\n",
    "    pbar.set_postfix({'loss': f'{loss.item():.4f}', 'epoch': current_epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:22:19.557224Z",
     "iopub.status.busy": "2025-07-26T13:22:19.556984Z",
     "iopub.status.idle": "2025-07-26T13:22:19.606972Z",
     "shell.execute_reply": "2025-07-26T13:22:19.606381Z",
     "shell.execute_reply.started": "2025-07-26T13:22:19.557208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: What is your name?\n",
      "HI: क्या आप क्या कर सकते हैं ?\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model_epoch5.pt\"  \n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "id2word_hi = {v: k for k, v in hi_vocab.items()}\n",
    "\n",
    "\n",
    "def translate_sentence(model, sentence, eng_vocab, hi_vocab, max_len=30):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = pad_tensor(encode_sentence(sentence, eng_vocab)).unsqueeze(0).to(device)\n",
    "        trg = torch.tensor([[hi_vocab[\"<sos>\"]]]).to(device)\n",
    "        translated = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output = model(src, trg)\n",
    "            next_token = output[0, -1].argmax().item()\n",
    "\n",
    "            if next_token == hi_vocab[\"<eos>\"]:\n",
    "                break\n",
    "\n",
    "            translated.append(next_token)\n",
    "            trg = torch.cat([trg, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "\n",
    "        return \" \".join([id2word_hi.get(idx, \"<unk>\") for idx in translated])\n",
    "\n",
    "\n",
    "test_sentence = \"What is your name?\"\n",
    "translation = translate_sentence(model, test_sentence, eng_vocab, hi_vocab)\n",
    "print(\"EN:\", test_sentence)\n",
    "print(\"HI:\", translation)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
